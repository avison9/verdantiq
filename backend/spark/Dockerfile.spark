FROM bitnami/spark:3.5.0

# Switch to root user to install dependencies
USER root

# Install required dependencies, including wget
RUN apt-get update && \
    apt-get install -y curl python3 python3-distutils wget && \
    ln -sf /usr/bin/python3 /usr/bin/python && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

RUN rm -f /opt/bitnami/spark/jars/hadoop-* 

RUN rm -f /opt/bitnami/spark/jars/guava-* 

# RUN rm -f /opt/bitnami/spark/jars/guava-14.0.1.jar

RUN rm -f /opt/bitnami/spark/jars/hadoop-shaded-guava-*.jar

# Add back ONLY the versions we need
COPY backend/libs/jdbc/hadoop-aws-3.3.6.jar \
     ./backend/libs/jdbc/hadoop-client-api-3.3.6.jar \
     ./backend/libs/jdbc/hadoop-client-runtime-3.3.6.jar \
     ./backend/libs/jdbc/guava-30.1-jre.jar \
     ./backend/libs/jdbc/hadoop-aws-3.3.6.jar \
     ./backend/libs/jdbc/hadoop-client-api-3.3.6.jar \
     ./backend/libs/jdbc/hadoop-client-runtime-3.3.6.jar \
     ./backend/libs/jdbc/iceberg-spark-runtime-3.5_2.12-1.4.2.jar \
     ./backend/libs/jdbc/hadoop-common-3.3.6.jar \
     /opt/bitnami/spark/jars/



# Download Guava JAR file to the correct location
# RUN wget https://repo1.maven.org/maven2/com/google/guava/guava/30.1-jre/guava-30.1-jre.jar -P /opt/bitnami/spark/jars

# Ensure correct permissions on the downloaded JAR file
# RUN chmod 644 /opt/bitnami/spark/jars/guava-30.1-jre.jar


# Set the Python environment variables for PySpark
ENV PYSPARK_PYTHON=/usr/bin/python3
ENV PYSPARK_DRIVER_PYTHON=/usr/bin/python3
ENV SPARK_HOME=/opt/bitnami/spark

# Switch back to the non-root user
USER 1001



