FROM apache/spark:3.5.0

USER root

RUN getent group spark || groupadd -g 1001 spark && \
    id -u spark &>/dev/null || useradd -u 1001 -g spark spark && \
    mkdir -p /opt/spark/work-dir && \
    chown -R spark:spark /opt/spark

# Installing dependencies
RUN apt-get update && \
    apt-get install -y curl python3 python3-distutils wget && \
    ln -sf /usr/bin/python3 /usr/bin/python && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

RUN rm -f /opt/spark/jars/guava-*.jar

RUN rm -f /opt/spark/jars/hadoo*.jar


COPY --chown=spark:spark backend/libs/jdbc/*  /opt/spark/jars/


COPY --chown=spark:spark backend/spark/config/configuration.conf /opt/spark/conf/spark-defaults.conf


ENV PYSPARK_PYTHON=/usr/bin/python3
ENV PYSPARK_DRIVER_PYTHON=/usr/bin/python3
ENV SPARK_HOME=/opt/spark
ENV HADOOP_USER_NAME=spark
ENV HADOOP_PROXY_USER=spark
ENV HADOOP_SECURITY_AUTHENTICATION=simple

COPY backend/spark/spark-entrypoint.sh /entrypoint.sh

COPY backend/spark/metrics.properties /opt/spark/conf/metrics.properties

RUN chmod +x /entrypoint.sh

USER spark

ENTRYPOINT ["/entrypoint.sh"]
