FROM apache/spark:3.5.0

USER root

RUN getent group spark || groupadd -g 1001 spark && \
    id -u spark &>/dev/null || useradd -u 1001 -g spark spark && \
    mkdir -p /opt/spark/work-dir && \
    chown -R spark:spark /opt/spark

# Installing dependencies
RUN apt-get update && \
    apt-get install -y curl python3 python3-distutils wget && \
    ln -sf /usr/bin/python3 /usr/bin/python && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

RUN rm -f /opt/spark/jars/guava-*.jar

# RUN rm -f /opt/spark/jars/hadoo-*.jar


COPY --chown=spark:spark backend/libs/jdbc/iceberg-spark-runtime-3.5_2.12-1.4.2.jar \
     backend/libs/jdbc/aws-java-sdk-bundle-1.12.100.jar \
     backend/libs/jdbc/guava-30.1.1-jre.jar \
     backend/libs/jdbc/hadoop-aws-3.3.6.jar \
     backend/libs/jdbc/hadoop-common-3.3.6.jar \
     backend/libs/jdbc/hadoop-client-api-3.3.6.jar \
     backend/libs/jdbc/hadoop-client-runtime-3.3.6.jar \
     backend/libs/jdbc/hadoop-yarn-server-web-proxy-3.3.6.jar \
     /opt/spark/jars/


COPY --chown=spark:spark backend/spark/config/configuration.conf /opt/spark/conf/spark-defaults.conf


ENV PYSPARK_PYTHON=/usr/bin/python3
ENV PYSPARK_DRIVER_PYTHON=/usr/bin/python3
ENV SPARK_HOME=/opt/spark
ENV HADOOP_USER_NAME=spark
ENV HADOOP_PROXY_USER=spark
ENV HADOOP_SECURITY_AUTHENTICATION=simple

COPY backend/spark/spark-entrypoint.sh /entrypoint.sh

RUN chmod +x /entrypoint.sh

USER spark

ENTRYPOINT ["/entrypoint.sh"]
